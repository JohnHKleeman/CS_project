{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 0.19.1\n",
      "numpy: 1.13.3\n",
      "pandas: 0.22.0\n",
      "matplotlib: 2.1.0\n",
      "sklearn: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import sys, pdb, warnings, scipy, matplotlib, sklearn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cPickle as pkl\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import cross_validation #might be model_selection <--- this is the new one\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import preprocessing \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sys.setrecursionlimit(15000)\n",
    "%matplotlib inline\n",
    "\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__)) \n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "\n",
    "#our modules see: CS_Project/cspython directory\n",
    "from cspython.scraper import modifiedSoup\n",
    "from cspython.data_processing import process_scrapped\n",
    "import cspython.analysis as a\n",
    "\n",
    "#import xlrd\n",
    "#import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook: Create a dataset of aggregate team statistics with a resolution of 1 row per team per match\n",
    "\n",
    "   ## There are 4 sections:\n",
    "       1 Data Processing functions, which complete the processing step of the raw scraped data\n",
    "       2 Data Agggregation functions, which aggregate the data to the team level for each match\n",
    "       3 Data set creation: calls the functions from the previous sections to create the data\n",
    "           note: the data are temporaly related, the statistics in every row are based on the \n",
    "           current match, and all previous matches (for which we have data)\n",
    "       4 Create X y functions, which will be used to create test train sets for cross validation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Data Processing functions\n",
    "\n",
    "    creates a giant dataframe containing all of the information scraped by the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_dfs(overview, big_data):\n",
    "    first=True\n",
    "    cols = ['map','round_num','half','match_id','series_id','ending','CT','T','side_winner','winner','team_A','team_B','team_A_score','team_B_score','match_num','team_players','K-D','+/-','ADR','KAST','Rating2.0','nicknames']\n",
    "    dfs = []\n",
    "    for idx, series_data in big_data.iteritems():\n",
    "        series_data_m = merge_matches(series_data)\n",
    "        series_data_m[\"date\"] = overview.loc[overview.id == idx, 'date']\n",
    "        series_data_mo = merge_overview(series_data_m, series_data)\n",
    "        series_data_mos = merge_scoreboards(series_data_mo, series_data)\n",
    "        series_data_mosm = match_data_board_changer(series_data_mos, series_data)\n",
    "        dfs.append(series_data_mosm)    \n",
    "        new_cols = list(set(series_data_mosm.columns) -set(cols))\n",
    "        cols += new_cols\n",
    "    data = pd.concat(dfs)\n",
    "    data = data.loc[:,cols]\n",
    "    data = data.reset_index()\n",
    "    return data\n",
    "\n",
    "def merge_matches(series_data):\n",
    "    dfs = []\n",
    "    for d in range(0,len(series_data['matches'])):\n",
    "        if d == 0:\n",
    "            series_data['matches'][d].rename(index = str, columns={ series_data['matches'][d].columns[10] : \"team_A\", series_data['matches'][d].columns[11] : \"team_B\" }, inplace=True)\n",
    "            dfs.append(series_data['matches'][d])\n",
    "        else:\n",
    "            series_data['matches'][d].rename(index = str, columns={ series_data['matches'][d].columns[10] : \"team_A\", series_data['matches'][d].columns[11] : \"team_B\" }, inplace=True)\n",
    "            dfs.append(series_data['matches'][d])\n",
    "    series_data_m = pd.concat(dfs,ignore_index=True)\n",
    "    return series_data_m\n",
    "    #should work for concact the matches together\n",
    "    \n",
    "def merge_overview(series_data_m, series_data):\n",
    "    series_data['match_overview']['team_A_name'] = series_data['match_overview'].columns[4]\n",
    "    series_data['match_overview']['team_B_name'] = series_data['match_overview'].columns[5]\n",
    "    \n",
    "    series_data['match_overview'].loc[(series_data['match_overview']['winner'] == series_data['match_overview'].columns[4]),'loser_of_match'] = series_data['match_overview'].team_B_name\n",
    "    series_data['match_overview'].loc[(series_data['match_overview']['winner'] != series_data['match_overview'].columns[4]),'loser_of_match'] = series_data['match_overview'].team_A_name\n",
    "    series_data['match_overview'] = series_data['match_overview'].rename(index = str, columns ={series_data['match_overview'].columns[3]: \"winner_of_match\",series_data['match_overview'].columns[4]: \"team_A_score\",series_data['match_overview'].columns[5]: \"team_B_score\"})\n",
    "    \n",
    "    \n",
    "    series_data_mo = pd.merge(series_data_m, series_data['match_overview'], on=['match_id', 'map', 'series_id'])\n",
    "    \n",
    "    return series_data_mo\n",
    "#works at merging matches with match_overviewb\n",
    "\n",
    "def merge_scoreboards(series_data_mo, series_data):    \n",
    "   \n",
    "    for i in range(len(series_data['scoreboards'][0])):\n",
    "        series_data['scoreboards'][0][i]['match_num'] = i+1\n",
    "        #pdb.set_trace()\n",
    "        series_data['scoreboards'][0][i]['player_team_name'] = series_data_mo.loc[(series_data_mo['match_num']== i+1),'team_A_name'].unique()[0]\n",
    "        series_data['scoreboards'][0][i] = series_data['scoreboards'][0][i].rename(index = str, columns={ series_data['scoreboards'][0][i].columns[0] : \"team_players\" })\n",
    "        \n",
    "        series_data['scoreboards'][1][i]['match_num'] = i+1\n",
    "        series_data['scoreboards'][1][i]['player_team_name'] = series_data_mo.loc[(series_data_mo['match_num']== i+1),'team_B_name'].unique()[0]\n",
    "        series_data['scoreboards'][1][i] = series_data['scoreboards'][1][i].rename(index = str, columns={ series_data['scoreboards'][1][i].columns[0] : \"team_players\"})\n",
    "        \n",
    "        new_df = pd.concat([series_data['scoreboards'][0][i], series_data['scoreboards'][1][i]])\n",
    "        \n",
    "        if i == 0:\n",
    "            con_df = new_df\n",
    "        else:\n",
    "            con_df = pd.concat([con_df, new_df])\n",
    "   \n",
    "    series_data_mos = pd.merge(series_data_mo, con_df, how='outer', on='match_num')\n",
    "    series_data_mos['nicknames'] = series_data_mos['team_players'].str.split(expand = True)[3]\n",
    "    return series_data_mos\n",
    "\n",
    "\n",
    "def match_data_board_changer(series_data_mos, series_data):\n",
    "    board_name = ['first_kills','who_kill_who', 'awp_kills']\n",
    "    for idx, a in enumerate(series_data['match_data']):\n",
    "        new_df = pd.DataFrame()\n",
    "        names_c = a['first_kills'].columns[1:]\n",
    "        for idx1, c in enumerate(board_name):\n",
    "            a[c].set_index('Unnamed: 0', inplace=True)\n",
    "            new_board_c = pd.DataFrame()\n",
    "            new_board_r= pd.DataFrame()\n",
    "            for b in names_c:\n",
    "                new_board_c[b+'_'+c] = a[c][b].str.split(':', expand=True)[0]    \n",
    "            names_r = a[c].T.columns\n",
    "            for b in names_r:\n",
    "                new_board_r[b+'_'+c] = a[c].T[b].str.split(pat = ':', expand = True)[1]\n",
    "            new_board_c['nicknames'] = new_board_c.index\n",
    "            new_board_r['nicknames'] = new_board_r.index\n",
    "            board_df = new_board_c.append(new_board_r)\n",
    "            if idx1 == 0:\n",
    "                new_df = board_df\n",
    "            else:\n",
    "                new_df = pd.merge(new_df, board_df, on = 'nicknames')\n",
    "        if idx == 0:\n",
    "            new_df['match_num'] = 1+idx\n",
    "            con_df = new_df\n",
    "        else:\n",
    "            new_df['match_num'] = 1+idx\n",
    "            try:\n",
    "                con_df = con_df.append(new_df, ignore_index=True)\n",
    "            except:\n",
    "                print con_df.columns\n",
    "                print new_df.columns\n",
    "    con_df = con_df.loc[:, ~con_df.columns.duplicated()]\n",
    "    series_data_mosm = pd.merge(series_data_mos, con_df, on=['nicknames','match_num'])\n",
    "    return series_data_mosm\n",
    "\n",
    "#works at adding match_num to scoreboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aggregation functions\n",
    "    takes the data created by above functions, where each row is 1 player in 1 match, and aggregates \n",
    "    the player data so each row is 1 team in match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_fwa_dr_columns(data, col_list):  # first k , awp, who, divided by rounds \n",
    "    columns = pd.Series(data.columns)\n",
    "    for a in col_list:\n",
    "        col = columns[columns.str.contains(a)]\n",
    "        data[a+'_sum_dr'] = data[col].convert_objects(convert_numeric = True).sum(axis = 1) / (data['team_A_score'] + data['team_B_score'])\n",
    "    r = data.loc[:, data.columns != 'date'].apply(pd.to_numeric, errors='ignore')\n",
    "    r['date'] = data.date\n",
    "    return r\n",
    "        \n",
    "def create_fwadr_his(data, col_list):   # column with historic awp, first ,who_killwho of player vs player. \n",
    "    match_id = data.match_id.unique()\n",
    "    players = data.nicknames.unique()\n",
    "    for a in col_list:\n",
    "        grouping = data.groupby(['nicknames', 'match_id']).mean().loc[:,a+'_sum_dr']\n",
    "        grouping = pd.DataFrame(grouping)\n",
    "        grouping = grouping.reset_index()\n",
    "        grouping = grouping.groupby(['nicknames'])[a+'_sum_dr'].sum()/ grouping.groupby(['nicknames'])[a+'_sum_dr'].count()\n",
    "        grouping = pd.DataFrame(grouping)\n",
    "        grouping = grouping.reset_index()\n",
    "        grouping = grouping.rename(index=str, columns={a +'_sum_dr': a + '_sum_dr_hist'})\n",
    "        data = pd.merge(data, grouping, on = 'nicknames')\n",
    "        \n",
    "    return data\n",
    "\n",
    "def create_matches_count(data): # how many matches a person has played\n",
    "    grouping = data.groupby(['nicknames', 'match_id'])['ADR'].count()\n",
    "    grouping = pd.DataFrame(grouping)\n",
    "    grouping = grouping.reset_index()\n",
    "    grouping = grouping.groupby(['nicknames'])['ADR'].count()\n",
    "    grouping = pd.DataFrame(grouping)\n",
    "    grouping = grouping.reset_index()\n",
    "    grouping = grouping.rename(index=str, columns={'ADR': 'matches_played_player'})\n",
    "    data = pd.merge(data, grouping, on = 'nicknames')\n",
    "    return data\n",
    "    \n",
    "def create_avdamage_his(data):  # column with historic average damage of individual \n",
    "    grouping = data.groupby(['nicknames', 'match_id'])['ADR'].mean() \n",
    "    grouping = pd.DataFrame(grouping)\n",
    "    grouping = grouping.reset_index()\n",
    "    grouping = grouping.groupby(['nicknames'])['ADR'].sum()/ grouping.groupby(['nicknames'])['ADR'].count()\n",
    "    grouping = pd.DataFrame(grouping)\n",
    "    grouping = grouping.reset_index()\n",
    "    grouping = grouping.rename(index=str, columns={'ADR': 'ADR_hist'})\n",
    "    data = pd.merge(data, grouping, on = 'nicknames')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_map_win_loss_his(data):  # team total win and loses on map with total times played on map\n",
    "    maps = data.map.unique()\n",
    "    teams = data.player_team_name.unique()\n",
    "    match_id = data.match_id.unique()\n",
    "    for a in maps:\n",
    "        data[a + \"_win_his\"] = 0\n",
    "        data[a + \"_loss_his\"] = 0\n",
    "        data[a + \"_total_played\"] = 0\n",
    "    for a in match_id:\n",
    "        map_for_match = data.loc[(data['match_id'] == a) ,'map'].unique()\n",
    "        winner_of_map = data.loc[(data['match_id'] == a), 'winner_of_match'].unique()\n",
    "        loser_of_map = data.loc[(data['match_id'] == a), 'loser_of_match'].unique()\n",
    "        data.loc[(data['player_team_name'] == winner_of_map[0]), [map_for_match[0] + \"_win_his\", map_for_match[0] +'_total_played']] += 1\n",
    "        data.loc[(data['player_team_name'] == loser_of_map[0]), [map_for_match[0] + \"_loss_his\", map_for_match[0] +'_total_played']] += 1 \n",
    "       \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_map_win_his_per(data): #percentage team total win and loses on map\n",
    "    teams = data.player_team_name.unique()\n",
    "    maps = data.map.unique()\n",
    "    for a in maps:\n",
    "        data[a + '_win_perc_map'] = 0\n",
    "        for b in teams:\n",
    "            pg = (data.player_team_name == b) \n",
    "            data.loc[pg,a + '_win_perc_map'] = data.loc[pg, a + \"_win_his\"].unique()[0] / float((data.loc[pg, a + \"_win_his\"].unique()[0] + data.loc[pg, a + \"_loss_his\"].unique()[0])) \n",
    "    data = data.fillna(0)        \n",
    "    return data    \n",
    "\n",
    "def create_rounds_won_vs_team_his(data): # team rounds won vs another team\n",
    "    grouping = data.groupby(['match_id','map','team_A_name','team_B_name', 'team_A_score', 'team_B_score'])['round_num'].count()\n",
    "    grouping = pd.DataFrame(grouping)\n",
    "    grouping = grouping.add_suffix('_Count').reset_index()\n",
    "    grouping = grouping.groupby(['team_A_name', 'team_B_name']).sum()\n",
    "    grouping = pd.DataFrame(grouping)\n",
    "    grouping = grouping.add_suffix('_Count').reset_index()\n",
    "    forward = grouping.team_A_name+grouping.team_B_name\n",
    "    reverse = grouping.team_B_name+grouping.team_A_name\n",
    "    for idx, val in enumerate(forward):\n",
    "        for idx2, val2 in enumerate(reverse):\n",
    "            if val == val2 and idx < idx2:\n",
    "                grouping.loc[idx,'team_A_score_Count'] += grouping.loc[idx2,'team_B_score_Count']\n",
    "                grouping.loc[idx,'team_B_score_Count'] += grouping.loc[idx2,'team_A_score_Count']\n",
    "                grouping.loc[idx2,'team_B_score_Count'] = grouping.loc[idx,'team_A_score_Count']\n",
    "                grouping.loc[idx2,'team_A_score_Count'] = grouping.loc[idx,'team_B_score_Count']\n",
    "            elif val == val2 and idx > idx2:\n",
    "                grouping.loc[idx2,'team_B_score_Count'] = grouping.loc[idx,'team_A_score_Count']\n",
    "                grouping.loc[idx2,'team_A_score_Count'] = grouping.loc[idx,'team_B_score_Count']\n",
    "    \n",
    "    grouping = grouping.drop('round_num_Count_Count', axis = 1)\n",
    "    col1 = list(grouping.team_A_name.unique())\n",
    "    col2  = list(grouping.team_B_name.unique())\n",
    "    col = col1 + col2\n",
    "    col = list(set(col))\n",
    "    data = pd.merge(data,grouping, on=['team_A_name', 'team_B_name']) \n",
    "    for a in col:\n",
    "        data['rd_total_his_'+ a] = 0\n",
    "        data.loc[(data.player_team_name != a) & (data.team_A_name == a) , 'rd_total_his_'+ a]=data.team_B_score_Count\n",
    "        data.loc[(data.player_team_name != a) & (data.team_B_name == a) , 'rd_total_his_'+ a]=data.team_A_score_Count\n",
    "        bgrouping = data.groupby(['player_team_name'])['rd_total_his_'+ a].max()\n",
    "        bgrouping = pd.DataFrame(bgrouping)\n",
    "        bgrouping = bgrouping.reset_index()\n",
    "        data = data.drop('rd_total_his_'+ a, axis = 1)\n",
    "        data = pd.merge(data, bgrouping, on = 'player_team_name')\n",
    "       \n",
    "    return data  \n",
    "\n",
    "def create_total_team_rd_map_his(data):\n",
    "    grouping = data.groupby(['map','team_A_name', 'team_A_score'])['round_num'].count()\n",
    "    grouping = pd.DataFrame(grouping)\n",
    "    grouping = grouping.add_suffix('_Count').reset_index()\n",
    "    grouping = grouping.groupby(['map','team_A_name'])[ 'team_A_score'].sum()\n",
    "    grouping = pd.DataFrame(grouping)\n",
    "    grouping = grouping.add_suffix('_Count').reset_index()\n",
    "    fgrouping = data.groupby(['player_team_name','map','team_B_name', 'team_B_score'])['round_num'].count()\n",
    "    fgrouping = pd.DataFrame(fgrouping)\n",
    "    fgrouping = fgrouping.add_suffix('_Count').reset_index()\n",
    "    fgrouping = fgrouping.groupby(['map','team_B_name'])[ 'team_B_score'].sum()\n",
    "    fgrouping = pd.DataFrame(fgrouping)\n",
    "    fgrouping = fgrouping.add_suffix('_Count').reset_index()\n",
    "    fgrouping = fgrouping.rename(index=str, columns={\"team_B_name\": \"team_A_name\", 'team_B_score_Count': 'team_A_score_Count'})\n",
    "    merged = pd.concat([grouping, fgrouping], axis = 0)\n",
    "    merged.groupby(['map', 'team_A_name'])['team_A_score_Count'].sum()\n",
    "    merged = pd.DataFrame(merged)\n",
    "    merged = merged.reset_index(drop = True)\n",
    "    merged = merged.rename(index=str, columns={\"team_A_name\": \"player_team_name\", \"team_A_score_Count\": 'total_team_rd_map'})\n",
    "    merged = merged.groupby(['player_team_name', 'map']).sum()\n",
    "    merged = pd.DataFrame(merged)\n",
    "    merged = merged.reset_index()\n",
    "    for a in list(merged.map.unique()):\n",
    "        merged.loc[:,'total_team_rd_'+ a] = 0\n",
    "        merged.loc[(merged.loc[:, 'map'] == a), 'total_team_rd_'+ a] = merged.loc[:,'total_team_rd_map']\n",
    "        ok_map = merged.groupby(['player_team_name'])['total_team_rd_'+ a].max()\n",
    "        ok_map = pd.DataFrame(ok_map)\n",
    "        ok_map = ok_map.reset_index()\n",
    "        data = pd.merge(data, ok_map, on = 'player_team_name')\n",
    "    return data\n",
    "    \n",
    "def create_faw_map_his(data, col_list):\n",
    "    for a in col_list:\n",
    "        fk_map = data.groupby(['match_id','nicknames', 'map'])[a+'_sum_dr'].mean()\n",
    "        fk_map = pd.DataFrame(fk_map)\n",
    "        fk_map = fk_map.reset_index()\n",
    "        fk_map = fk_map.groupby(['nicknames', 'map'])[a+'_sum_dr'].mean()\n",
    "        fk_map = pd.DataFrame(fk_map)\n",
    "        fk_map = fk_map.reset_index()\n",
    "        for b in list(fk_map.map.unique()):\n",
    "            fk_map.loc[:,a+'_'+b +'_dr_hist'] = 0 \n",
    "            fk_map.loc[(fk_map.loc[:, 'map'] == b), a+'_'+b+'_dr_hist'] = fk_map.loc[:,a + '_sum_dr']\n",
    "            ok_map = fk_map.groupby(['nicknames'])[a+'_'+b+'_dr_hist'].max()\n",
    "            ok_map = pd.DataFrame(ok_map)\n",
    "            ok_map = ok_map.reset_index()\n",
    "            data = pd.merge(data, ok_map, on = 'nicknames')\n",
    "    return data\n",
    "\n",
    "def create_avdamage_map_his(data):# historic average damage of individual for each map\n",
    "    map_adr = data.groupby(['match_id','nicknames', 'map'])['ADR'].mean()\n",
    "    map_adr = pd.DataFrame(map_adr)\n",
    "    map_adr = map_adr.reset_index()\n",
    "    map_adr = map_adr.groupby(['nicknames', 'map'])['ADR'].mean()\n",
    "    map_adr = pd.DataFrame(map_adr)\n",
    "    map_adr = map_adr.reset_index()\n",
    "    for a in list(map_adr.map.unique()):\n",
    "        map_adr.loc[:,'ADR_his_'+ a] = 0\n",
    "        map_adr.loc[(map_adr.loc[:, 'map'] == a), 'ADR_his_'+ a] = map_adr.loc[:,'ADR']\n",
    "        ok_map = map_adr.groupby(['nicknames'])['ADR_his_'+ a].max()\n",
    "        ok_map = pd.DataFrame(ok_map)\n",
    "        ok_map = ok_map.reset_index()\n",
    "        data = pd.merge(data, ok_map, on = 'nicknames')\n",
    "    return data\n",
    "\n",
    "def create_opponent_team_col(data):\n",
    "    data.loc[:,'player_team_opponent'] = np.nan\n",
    "    data.loc[(data['team_A_name'] != data['player_team_name']),'player_team_opponent'] = data.loc[:,'team_A_name']\n",
    "    data.loc[(data['team_B_name'] != data['player_team_name']),'player_team_opponent'] = data.loc[:,'team_B_name']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_slice(data, col_list):\n",
    "    data = create_fwa_dr_columns(data, col_list)\n",
    "    data = create_fwadr_his(data,col_list)\n",
    "    data = create_avdamage_his(data)\n",
    "    data = create_map_win_loss_his(data)\n",
    "    data = create_map_win_his_per(data)\n",
    "    data = create_rounds_won_vs_team_his(data)\n",
    "    data = create_total_team_rd_map_his(data)\n",
    "    data = create_avdamage_map_his(data)\n",
    "    data = create_faw_map_his(data, col_list)\n",
    "    data = create_matches_count(data)\n",
    "    data = create_opponent_team_col(data)\n",
    "    \n",
    "    data_adv = data.loc[:, 'first_kills_sum_dr_hist':'player_team_opponent']\n",
    "    data_adv['match_id'] = data['match_id']\n",
    "    data_adv['player_team_name'] = data['player_team_name']\n",
    "    data_adv['map'] = data['map']\n",
    "    data_adv = data_adv.drop(['team_A_score_Count','team_B_score_Count'], axis = 1)\n",
    "    data_adv = data_adv.groupby(['match_id', 'player_team_name', 'player_team_opponent','map']).mean()\n",
    "    data_adv = pd.DataFrame(data_adv)\n",
    "    data_adv = data_adv.reset_index()\n",
    "    data_adv = data_adv.apply(pd.to_numeric, errors='ignore')\n",
    "    return data_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-20 16:10:00\n",
      "cur_slice:  490\n",
      "cur_slice matches:  2\n",
      "4\n",
      "2017-05-20 19:10:00\n",
      "cur_slice:  1010\n",
      "cur_slice matches:  4\n",
      "8\n",
      "filtered:  4\n",
      "2017-05-20 22:00:00\n",
      "cur_slice:  1703\n",
      "cur_slice matches:  7\n",
      "14\n",
      "filtered:  6\n",
      "2017-05-21 01:15:00\n",
      "cur_slice:  2189\n",
      "cur_slice matches:  9\n",
      "18\n",
      "filtered:  4\n",
      "2017-05-21 20:00:00\n",
      "cur_slice:  2909\n",
      "cur_slice matches:  12\n",
      "24\n",
      "filtered:  6\n",
      "2017-05-21 20:15:00\n",
      "cur_slice:  3449\n",
      "cur_slice matches:  14\n",
      "28\n",
      "filtered:  4\n",
      "2017-05-21 23:15:00\n",
      "cur_slice:  4179\n",
      "cur_slice matches:  17\n",
      "34\n",
      "filtered:  6\n",
      "2017-05-22 02:30:00\n",
      "cur_slice:  4999\n",
      "cur_slice matches:  20\n",
      "40\n",
      "filtered:  6\n",
      "2017-05-23 22:15:00\n",
      "cur_slice:  5233\n",
      "cur_slice matches:  21\n",
      "42\n",
      "filtered:  2\n",
      "2017-05-24 00:20:00\n",
      "cur_slice:  5803\n",
      "cur_slice matches:  23\n",
      "46\n",
      "filtered:  4\n",
      "2017-05-24 22:00:00\n",
      "cur_slice:  6103\n",
      "cur_slice matches:  24\n",
      "48\n",
      "filtered:  2\n",
      "2017-05-24 23:30:00\n",
      "cur_slice:  6403\n",
      "cur_slice matches:  25\n",
      "50\n",
      "filtered:  2\n",
      "2017-05-25 00:40:00\n",
      "cur_slice:  6673\n",
      "cur_slice matches:  26\n",
      "52\n",
      "filtered:  2\n",
      "2017-05-25 01:25:00\n",
      "cur_slice:  6893\n",
      "cur_slice matches:  27\n",
      "54\n",
      "filtered:  2\n",
      "2017-05-25 01:45:00\n",
      "cur_slice:  7127\n",
      "cur_slice matches:  28\n",
      "56\n",
      "filtered:  2\n",
      "2017-05-25 22:20:00\n",
      "cur_slice:  7351\n",
      "cur_slice matches:  29\n",
      "58\n",
      "filtered:  2\n",
      "2017-05-25 23:35:00\n",
      "cur_slice:  7559\n",
      "cur_slice matches:  30\n",
      "60\n",
      "filtered:  2\n",
      "2017-05-26 00:25:00\n",
      "cur_slice:  7759\n",
      "cur_slice matches:  31\n",
      "62\n",
      "filtered:  2\n",
      "2017-05-26 01:15:00\n",
      "cur_slice:  8039\n",
      "cur_slice matches:  32\n",
      "64\n",
      "filtered:  2\n",
      "2017-05-26 22:10:00\n",
      "cur_slice:  8264\n",
      "cur_slice matches:  33\n",
      "66\n",
      "filtered:  2\n",
      "2017-05-26 23:15:00\n",
      "cur_slice:  8480\n",
      "cur_slice matches:  34\n",
      "68\n",
      "filtered:  2\n",
      "2017-05-27 00:05:00\n",
      "cur_slice:  8723\n",
      "cur_slice matches:  35\n",
      "70\n",
      "filtered:  2\n",
      "2017-05-27 01:00:00\n",
      "cur_slice:  8975\n",
      "cur_slice matches:  36\n",
      "72\n",
      "filtered:  2\n",
      "2017-05-27 18:00:00\n",
      "cur_slice:  9175\n",
      "cur_slice matches:  37\n",
      "74\n",
      "filtered:  2\n",
      "2017-05-27 18:50:00\n",
      "cur_slice:  9465\n",
      "cur_slice matches:  38\n",
      "76\n",
      "filtered:  2\n",
      "2017-05-27 20:25:00\n",
      "cur_slice:  9735\n",
      "cur_slice matches:  39\n",
      "78\n",
      "filtered:  2\n",
      "2017-05-27 21:35:00\n",
      "cur_slice:  9915\n",
      "cur_slice matches:  40\n",
      "80\n",
      "filtered:  2\n",
      "2017-05-28 18:10:00\n",
      "cur_slice:  10149\n",
      "cur_slice matches:  41\n",
      "82\n",
      "filtered:  2\n",
      "2017-05-28 19:00:00\n",
      "cur_slice:  10410\n",
      "cur_slice matches:  42\n",
      "84\n",
      "filtered:  2\n",
      "2017-05-28 20:00:00\n",
      "cur_slice:  10680\n",
      "cur_slice matches:  43\n",
      "86\n",
      "filtered:  2\n",
      "2017-05-28 21:30:00\n",
      "cur_slice:  10905\n",
      "cur_slice matches:  44\n",
      "88\n",
      "filtered:  2\n",
      "2017-05-30 16:30:00\n",
      "cur_slice:  11112\n",
      "cur_slice matches:  45\n",
      "90\n",
      "filtered:  2\n",
      "2017-05-30 18:55:00\n",
      "cur_slice:  11301\n",
      "cur_slice matches:  46\n",
      "92\n",
      "filtered:  2\n",
      "2017-05-30 23:15:00\n",
      "cur_slice:  11601\n",
      "cur_slice matches:  47\n",
      "94\n",
      "filtered:  2\n",
      "2017-05-31 00:45:00\n",
      "cur_slice:  11790\n",
      "cur_slice matches:  48\n",
      "96\n",
      "filtered:  2\n",
      "2017-05-31 01:45:00\n",
      "cur_slice:  12051\n",
      "cur_slice matches:  49\n",
      "98\n",
      "filtered:  2\n",
      "2017-05-31 18:45:00\n",
      "cur_slice:  12291\n",
      "cur_slice matches:  50\n",
      "100\n",
      "filtered:  2\n",
      "2017-06-06 22:25:00\n",
      "cur_slice:  12551\n",
      "cur_slice matches:  51\n",
      "102\n",
      "filtered:  2\n",
      "2017-06-06 23:40:00\n",
      "cur_slice:  12751\n",
      "cur_slice matches:  52\n",
      "104\n",
      "filtered:  2\n",
      "2017-06-07 00:30:00\n",
      "cur_slice:  13041\n",
      "cur_slice matches:  53\n",
      "106\n",
      "filtered:  2\n",
      "2017-06-07 01:30:00\n",
      "cur_slice:  13331\n",
      "cur_slice matches:  54\n",
      "108\n",
      "filtered:  2\n",
      "2017-06-07 22:00:00\n",
      "cur_slice:  13484\n",
      "cur_slice matches:  55\n",
      "110\n",
      "filtered:  2\n",
      "2017-06-07 22:45:00\n",
      "cur_slice:  13709\n",
      "cur_slice matches:  56\n",
      "112\n",
      "filtered:  2\n",
      "2017-06-08 00:00:00\n",
      "cur_slice:  13979\n",
      "cur_slice matches:  57\n",
      "114\n",
      "filtered:  2\n",
      "2017-06-08 01:00:00\n",
      "cur_slice:  14279\n",
      "cur_slice matches:  58\n",
      "116\n",
      "filtered:  2\n",
      "2017-06-08 02:20:00\n",
      "cur_slice:  14499\n",
      "cur_slice matches:  59\n",
      "118\n",
      "filtered:  2\n",
      "2017-06-08 03:10:00\n",
      "cur_slice:  14739\n",
      "cur_slice matches:  60\n",
      "120\n",
      "filtered:  2\n",
      "2017-06-08 17:05:00\n",
      "cur_slice:  14979\n",
      "cur_slice matches:  61\n",
      "122\n",
      "filtered:  2\n",
      "2017-06-08 18:30:00\n",
      "cur_slice:  15249\n",
      "cur_slice matches:  62\n",
      "124\n",
      "filtered:  2\n",
      "2017-06-08 20:10:00\n",
      "cur_slice:  15459\n",
      "cur_slice matches:  63\n",
      "126\n",
      "filtered:  2\n",
      "2017-06-08 21:35:00\n",
      "cur_slice:  15989\n",
      "cur_slice matches:  65\n",
      "130\n",
      "filtered:  4\n",
      "2017-06-09 00:55:00\n",
      "cur_slice:  16589\n",
      "cur_slice matches:  67\n",
      "134\n",
      "filtered:  4\n",
      "2017-06-09 18:30:00\n",
      "cur_slice:  16839\n",
      "cur_slice matches:  68\n",
      "136\n",
      "filtered:  2\n",
      "2017-06-09 20:10:00\n",
      "cur_slice:  17119\n",
      "cur_slice matches:  69\n",
      "138\n",
      "filtered:  2\n",
      "2017-06-10 17:00:00\n",
      "cur_slice:  17669\n",
      "cur_slice matches:  71\n",
      "142\n",
      "filtered:  4\n",
      "2017-06-10 19:50:00\n",
      "cur_slice:  18219\n",
      "cur_slice matches:  73\n",
      "146\n",
      "filtered:  4\n",
      "2017-06-10 22:50:00\n",
      "cur_slice:  18709\n",
      "cur_slice matches:  75\n",
      "150\n",
      "filtered:  4\n",
      "2017-06-11 17:00:00\n",
      "cur_slice:  19299\n",
      "cur_slice matches:  77\n",
      "154\n",
      "filtered:  4\n",
      "2017-06-11 20:10:00\n",
      "cur_slice:  19819\n",
      "cur_slice matches:  79\n",
      "158\n",
      "filtered:  4\n",
      "2017-06-11 23:25:00\n",
      "cur_slice:  20469\n",
      "cur_slice matches:  82\n",
      "164\n",
      "filtered:  6\n",
      "2017-06-17 17:00:00\n",
      "cur_slice:  20703\n",
      "cur_slice matches:  83\n",
      "166\n",
      "filtered:  2\n",
      "2017-06-18 00:15:00\n",
      "cur_slice:  21433\n",
      "cur_slice matches:  86\n",
      "172\n",
      "filtered:  6\n",
      "2017-06-18 03:20:00\n",
      "cur_slice:  22183\n",
      "cur_slice matches:  89\n",
      "178\n",
      "filtered:  6\n",
      "2017-06-18 20:15:00\n",
      "cur_slice:  22593\n",
      "cur_slice matches:  91\n",
      "182\n",
      "filtered:  4\n",
      "2017-06-18 22:50:00\n",
      "cur_slice:  23059\n",
      "cur_slice matches:  93\n",
      "186\n",
      "filtered:  4\n",
      "2017-06-19 01:10:00\n",
      "cur_slice:  23959\n",
      "cur_slice matches:  96\n",
      "192\n",
      "filtered:  6\n",
      "2017-06-19 10:00:00\n",
      "cur_slice:  24616\n",
      "cur_slice matches:  99\n",
      "198\n",
      "filtered:  6\n",
      "2017-06-25 21:05:00\n",
      "cur_slice:  25146\n",
      "cur_slice matches:  101\n",
      "202\n",
      "filtered:  4\n",
      "2017-06-25 23:20:00\n",
      "cur_slice:  25646\n",
      "cur_slice matches:  103\n",
      "206\n",
      "filtered:  4\n",
      "2017-06-26 03:00:00\n",
      "cur_slice:  26136\n",
      "cur_slice matches:  105\n",
      "210\n",
      "filtered:  4\n",
      "2017-07-01 08:20:00\n",
      "cur_slice:  26396\n",
      "cur_slice matches:  106\n",
      "212\n",
      "filtered:  2\n",
      "2017-07-04 20:50:00\n",
      "cur_slice:  26666\n",
      "cur_slice matches:  107\n",
      "214\n",
      "filtered:  2\n",
      "2017-07-05 18:25:00\n",
      "cur_slice:  26855\n",
      "cur_slice matches:  108\n",
      "216\n",
      "filtered:  2\n",
      "2017-07-06 12:25:00\n",
      "cur_slice:  27125\n",
      "cur_slice matches:  109\n",
      "218\n",
      "filtered:  2\n",
      "2017-07-07 17:55:00\n",
      "cur_slice:  27791\n",
      "cur_slice matches:  112\n",
      "224\n",
      "filtered:  6\n",
      "2017-07-09 14:00:00\n",
      "cur_slice:  28538\n",
      "cur_slice matches:  115\n",
      "230\n",
      "filtered:  6\n",
      "2017-07-12 04:20:00\n",
      "cur_slice:  29240\n",
      "cur_slice matches:  118\n",
      "236\n",
      "filtered:  6\n",
      "2017-07-13 12:25:00\n",
      "cur_slice:  29510\n",
      "cur_slice matches:  119\n",
      "238\n",
      "filtered:  2\n",
      "2017-07-19 17:25:00\n",
      "cur_slice:  29735\n",
      "cur_slice matches:  120\n",
      "240\n",
      "filtered:  2\n",
      "2017-07-21 22:05:00\n",
      "cur_slice:  29985\n",
      "cur_slice matches:  121\n",
      "242\n",
      "filtered:  2\n",
      "2017-08-19 22:10:00\n",
      "cur_slice:  30565\n",
      "cur_slice matches:  123\n",
      "246\n",
      "filtered:  4\n",
      "2017-08-20 19:00:00\n",
      "cur_slice:  31075\n",
      "cur_slice matches:  125\n",
      "250\n",
      "filtered:  4\n",
      "2017-08-22 23:00:00\n",
      "cur_slice:  31509\n",
      "cur_slice matches:  127\n",
      "254\n",
      "filtered:  4\n",
      "2017-08-23 00:05:00\n",
      "cur_slice:  31769\n",
      "cur_slice matches:  128\n",
      "256\n",
      "filtered:  2\n",
      "2017-08-23 00:10:00\n",
      "cur_slice:  31921\n",
      "cur_slice matches:  129\n",
      "258\n",
      "filtered:  2\n",
      "2017-08-23 01:15:00\n",
      "cur_slice:  32191\n",
      "cur_slice matches:  130\n",
      "260\n",
      "filtered:  2\n",
      "2017-08-23 01:20:00\n",
      "cur_slice:  32491\n",
      "cur_slice matches:  131\n",
      "262\n",
      "filtered:  2\n",
      "2017-08-23 02:30:00\n",
      "cur_slice:  32771\n",
      "cur_slice matches:  132\n",
      "264\n",
      "filtered:  2\n",
      "2017-08-23 03:20:00\n",
      "cur_slice:  32981\n",
      "cur_slice matches:  133\n",
      "266\n",
      "filtered:  2\n",
      "2017-08-23 23:00:00\n",
      "cur_slice:  33431\n",
      "cur_slice matches:  135\n",
      "270\n",
      "filtered:  4\n",
      "2017-08-24 00:10:00\n",
      "cur_slice:  33951\n",
      "cur_slice matches:  137\n",
      "274\n",
      "filtered:  4\n",
      "2017-08-24 01:20:00\n",
      "cur_slice:  34356\n",
      "cur_slice matches:  139\n",
      "278\n",
      "filtered:  4\n",
      "2017-08-24 02:25:00\n",
      "cur_slice:  34608\n",
      "cur_slice matches:  140\n",
      "280\n",
      "filtered:  2\n",
      "2017-08-24 02:30:00\n",
      "cur_slice:  34878\n",
      "cur_slice matches:  141\n",
      "282\n",
      "filtered:  2\n",
      "2017-08-24 23:05:00\n",
      "cur_slice:  35428\n",
      "cur_slice matches:  143\n",
      "286\n",
      "filtered:  4\n",
      "2017-08-25 00:10:00\n",
      "cur_slice:  35958\n",
      "cur_slice matches:  145\n",
      "290\n",
      "filtered:  4\n",
      "2017-08-25 01:25:00\n",
      "cur_slice:  36374\n",
      "cur_slice matches:  147\n",
      "294\n",
      "filtered:  4\n",
      "2017-08-25 02:30:00\n",
      "cur_slice:  36544\n",
      "cur_slice matches:  148\n",
      "296\n",
      "filtered:  2\n",
      "2017-08-25 02:50:00\n",
      "cur_slice:  36796\n",
      "cur_slice matches:  149\n",
      "298\n",
      "filtered:  2\n",
      "2017-08-25 23:00:00\n",
      "cur_slice:  37096\n",
      "cur_slice matches:  150\n",
      "300\n",
      "filtered:  2\n",
      "2017-08-25 23:20:00\n",
      "cur_slice:  37348\n",
      "cur_slice matches:  151\n",
      "302\n",
      "filtered:  2\n",
      "2017-08-26 00:25:00\n",
      "cur_slice:  37618\n",
      "cur_slice matches:  152\n",
      "304\n",
      "filtered:  2\n",
      "2017-08-26 00:35:00\n",
      "cur_slice:  37834\n",
      "cur_slice matches:  153\n",
      "306\n",
      "filtered:  2\n",
      "2017-08-26 01:30:00\n",
      "cur_slice:  38124\n",
      "cur_slice matches:  154\n",
      "308\n",
      "filtered:  2\n",
      "2017-08-26 01:35:00\n",
      "cur_slice:  38367\n",
      "cur_slice matches:  155\n",
      "310\n",
      "filtered:  2\n",
      "2017-08-26 02:45:00\n",
      "cur_slice:  38805\n",
      "cur_slice matches:  157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314\n",
      "filtered:  4\n",
      "2017-08-27 00:15:00\n",
      "cur_slice:  39035\n",
      "cur_slice matches:  158\n",
      "316\n",
      "filtered:  2\n",
      "2017-08-27 18:50:00\n",
      "cur_slice:  39555\n",
      "cur_slice matches:  160\n",
      "320\n",
      "filtered:  4\n",
      "2017-08-27 18:55:00\n",
      "cur_slice:  40055\n",
      "cur_slice matches:  162\n",
      "324\n",
      "filtered:  4\n",
      "2017-08-27 22:55:00\n",
      "cur_slice:  40545\n",
      "cur_slice matches:  164\n",
      "328\n",
      "filtered:  4\n",
      "2017-08-29 22:05:00\n",
      "cur_slice:  41065\n",
      "cur_slice matches:  166\n",
      "332\n",
      "filtered:  4\n",
      "2017-08-31 00:40:00\n",
      "cur_slice:  41625\n",
      "cur_slice matches:  168\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open('../scrapped_data/esl_teams.pkl', 'rb') as f: \n",
    "     d = pkl.load(f)\n",
    "\n",
    "big_data = process_scrapped(d)\n",
    "\"\"\"\n",
    "\n",
    "with open('big_data.pkl', 'rb') as f:\n",
    "    big_data = pkl.load(f)\n",
    "    \n",
    "data = combine_dfs(*big_data)\n",
    "overview = big_data[0]\n",
    "\n",
    "#add a date\n",
    "for idx, row in overview.iterrows():\n",
    "    series_id = row.id\n",
    "    date = row.date\n",
    "    data.loc[data.series_id == series_id, 'date'] = date\n",
    "\n",
    "# sort dates    \n",
    "data = data.sort_values('date')\n",
    "\n",
    "data.to_pickle(\"johns_dataset.pkl\")\n",
    "\n",
    "col_list = ['first_kills', 'who_kill_who','awp_kills']\n",
    "full_dataset = pd.DataFrame()\n",
    "first = True\n",
    "for date in data.date.unique():\n",
    "    print date\n",
    "    cur_slice = data.loc[data.date <= date,:]\n",
    "    print 'cur_slice: ', len(cur_slice)\n",
    "    print 'cur_slice matches: ', len(cur_slice.match_id.unique())\n",
    "    new_df = process_slice(cur_slice, col_list)\n",
    "    print len(new_df)\n",
    "    if not first:\n",
    "        new_df = new_df.loc[~new_df.match_id.isin(full_dataset.match_id),]\n",
    "        print 'filtered: ', len(new_df)\n",
    "    else:\n",
    "        first = False\n",
    "    full_dataset = pd.concat([full_dataset, new_df])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some profiling work to improve speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiling the aggregation steps\n",
    "import cProfile\n",
    "import pstats\n",
    "import cPickle as pkl\n",
    "\n",
    "with open('../scrapped_data/esl_teams.pkl', 'rb') as f: \n",
    "     d = pkl.load(f)\n",
    "\n",
    "big_data = process_scrapped(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 11 01:43:30 2018    processing_results\n",
      "\n",
      "         68828020 function calls (67952241 primitive calls) in 56.441 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 750 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.171    0.171   56.441   56.441 <string>:1(<module>)\n",
      "        1    0.215    0.215   56.270   56.270 <ipython-input-132-97f6c02af10c>:1(combine_dfs)\n",
      "      456    0.328    0.001   37.460    0.082 <ipython-input-132-97f6c02af10c>:70(match_data_board_changer)\n",
      "     5745    0.345    0.000   13.528    0.002 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.py:5170(concatenate_block_managers)\n",
      "     3169    0.016    0.000   11.526    0.004 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:21(concat)\n",
      "    27962    0.084    0.000   10.424    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:2505(__setitem__)\n",
      "    18582    0.047    0.000   10.003    0.001 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\strings.py:1478(split)\n",
      "    44128    0.266    0.000    9.862    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:316(__init__)\n",
      "    27962    0.105    0.000    9.852    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:2573(_set_item)\n",
      "     2576    0.018    0.000    9.547    0.004 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:47(merge)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats instance at 0x000000003F7BBCC8>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('big_data.pkl', 'rb') as f:\n",
    "    big_data = pkl.load(f)\n",
    "\n",
    "cProfile.run(\"combine_dfs(*big_data)\", 'processing_results')\n",
    "results = pstats.Stats('processing_results')\n",
    "results.sort_stats('cumulative').print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 11 01:13:31 2018    processing_results\n",
      "\n",
      "         75533 function calls (74371 primitive calls) in 0.059 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 521 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.059    0.059 <string>:1(<module>)\n",
      "        1    0.001    0.001    0.059    0.059 <ipython-input-120-8edafd16a9a2>:23(match_data_board_changer)\n",
      "       65    0.000    0.000    0.015    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:316(__init__)\n",
      "       37    0.000    0.000    0.014    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:2505(__setitem__)\n",
      "       30    0.000    0.000    0.014    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\strings.py:1478(split)\n",
      "       37    0.000    0.000    0.013    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:2573(_set_item)\n",
      "       30    0.000    0.000    0.012    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\strings.py:1394(_wrap_result)\n",
      "        3    0.000    0.000    0.011    0.004 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:47(merge)\n",
      "       37    0.000    0.000    0.009    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.py:1953(_set_item)\n",
      "       37    0.000    0.000    0.009    0.000 C:\\Users\\mckak\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.py:3936(set)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats instance at 0x000000001703EE88>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('big_data.pkl', 'rb') as f:\n",
    "    big_data = pkl.load(f)\n",
    "\n",
    "def combine_dfs(overview, big_data):\n",
    "    first=True\n",
    "    cols = ['map','round_num','half','match_id','series_id','ending','CT','T','side_winner','winner','team_A','team_B','team_A_score','team_B_score','match_num','team_players','K-D','+/-','ADR','KAST','Rating2.0','nicknames']\n",
    "    dfs = []\n",
    "    for idx, series_data in big_data.iteritems():\n",
    "        series_data_m = merge_matches(series_data)\n",
    "        series_data_m[\"date\"] = overview.loc[overview.id == idx, 'date']\n",
    "        series_data_mo = merge_overview(series_data_m, series_data)\n",
    "        series_data_mos = merge_scoreboards(series_data_mo, series_data)\n",
    "        return series_data_mos, series_data\n",
    "        series_data_mosm = match_data_board_changer(series_data_mos, series_data)\n",
    "        dfs.append(series_data_mosm)    \n",
    "        new_cols = list(set(series_data_mosm.columns) -set(cols))\n",
    "        cols += new_cols\n",
    "    data = pd.concat(dfs)\n",
    "    data = data.loc[:,cols]\n",
    "    data = data.reset_index()\n",
    "    return data\n",
    "\n",
    "def match_data_board_changer(series_data_mos, series_data):\n",
    "    board_name = ['first_kills','who_kill_who', 'awp_kills']\n",
    "    for idx, a in enumerate(series_data['match_data']):\n",
    "        new_df = pd.DataFrame()\n",
    "        names_c = a['first_kills'].columns[1:]\n",
    "        for idx1, c in enumerate(board_name):\n",
    "            a[c].set_index('Unnamed: 0', inplace=True)\n",
    "            new_board_c = pd.DataFrame()\n",
    "            new_board_r= pd.DataFrame()\n",
    "            for b in names_c:\n",
    "                new_board_c[b+'_'+c] = a[c][b].str.split(':', expand=True)[0]    \n",
    "            names_r = a[c].T.columns\n",
    "            for b in names_r:\n",
    "                new_board_r[b+'_'+c] = a[c].T[b].str.split(pat = ':', expand = True)[1]\n",
    "            new_board_c['nicknames'] = new_board_c.index\n",
    "            new_board_r['nicknames'] = new_board_r.index\n",
    "            board_df = new_board_c.append(new_board_r)\n",
    "            if idx1 == 0:\n",
    "                new_df = board_df\n",
    "            else:\n",
    "                new_df = pd.merge(new_df, board_df, on = 'nicknames')\n",
    "        if idx == 0:\n",
    "            new_df['match_num'] = 1+idx\n",
    "            con_df = new_df\n",
    "        else:\n",
    "            new_df['match_num'] = 1+idx\n",
    "            try:\n",
    "                con_df = con_df.append(new_df, ignore_index=True)\n",
    "            except:\n",
    "                print con_df.columns\n",
    "                print new_df.columns\n",
    "    con_df = con_df.loc[:, ~con_df.columns.duplicated()]\n",
    "    series_data_mosm = pd.merge(series_data_mos, con_df, on=['nicknames','match_num'])\n",
    "    return series_data_mosm\n",
    "\n",
    "\n",
    "series_data_mos, series_data = combine_dfs(*big_data)\n",
    "cProfile.run(\"match_data_board_changer(series_data_mos, series_data)\", 'processing_results')\n",
    "results = pstats.Stats('processing_results')\n",
    "results.sort_stats('cumulative').print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and appended new matches to full_dataset df once a day\n",
    "col_list = ['first_kills', 'who_kill_who','awp_kills']\n",
    "full_dataset = pd.DataFrame()\n",
    "first = True\n",
    "for date in data.date.unique():\n",
    "    print date\n",
    "    cur_slice = data.loc[data.date <= date,:]\n",
    "    new_df = process_slice(cur_slice, col_list)\n",
    "    if not first:\n",
    "        new_df = new_df.loc[~new_df.match_id.isin(full_dataset.match_id),]\n",
    "    else:\n",
    "        first = False\n",
    "    full_dataset = pd.concat([full_dataset, new_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating X and y functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datay = data.loc[:,['winner_of_match', 'player_team_name', 'match_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_adv = pd.get_dummies(data_adv,columns = ['player_team_name', 'player_team_opponent','map'])\n",
    "data_adv = data_adv.drop(['match_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datay.loc[datay.winner_of_match != datay.player_team_name, 'winner_of_match'] = 0\n",
    "datay.loc[datay.winner_of_match == datay.player_team_name, 'winner_of_match'] = 1\n",
    "datay.winner_of_match = datay.winner_of_match.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "datay = datay.groupby(['match_id', 'player_team_name'])['winner_of_match'].mean()\n",
    "datay = pd.DataFrame(datay)\n",
    "datay = datay.reset_index()\n",
    "datay = datay.drop(['player_team_name','match_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_adv = data_adv.drop(['Mirage_win_his','Mirage_loss_his','Train_win_his','Train_loss_his','Cobblestone_win_his'\n",
    "#                      ,'Cobblestone_loss_his','Cache_win_his','Cache_loss_his','Inferno_win_his','Inferno_loss_his'\n",
    "#                      ,'Overpass_win_his','Overpass_loss_his','Nuke_win_his','Nuke_loss_his'], axis = 1)\n",
    "\n",
    "\n",
    "# ,'who_kill_who_Cache_dr_hist'\n",
    "#                       ,'who_kill_who_Cobblestone_dr_hist','who_kill_who_Inferno_dr_hist','who_kill_who_Mirage_dr_hist'\n",
    "#                       ,'who_kill_who_Nuke_dr_hist','who_kill_who_Overpass_dr_hist','who_kill_who_Train_dr_hist'\n",
    "#                       , 'who_kill_who_sum_dr_hist'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_adv = data_adv.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = datay.values.astype(int)\n",
    "X = data_adv.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_cv(model, params):                             #KFOLD WITH GRID SEARCH\n",
    "    param_grid = params\n",
    "    kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "    grid_result = grid.fit(X, y)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    for params, mean_score, scores in grid_result.grid_scores_:\n",
    "        print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "num_instances = len(X) \n",
    "seed = 7\n",
    "scoring = 'roc_auc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = data_adv.iloc[:,top_56_important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.692009 (0.041771)\n",
      "LASSO: 0.605929 (0.047575)\n",
      "Ridge: 0.693587 (0.040757)\n",
      "LDA: 0.683520 (0.041036)\n",
      "NB: 0.673947 (0.044565)\n",
      "KNeighborsClassifier: 0.584792 (0.056266)\n",
      "XGBClassifier: 0.691801 (0.041618)\n",
      "GradientBoostingClassifier: 0.676411 (0.037104)\n",
      "AdaBoostClassifier: 0.692027 (0.045368)\n",
      "RandomForestClassifier: 0.605450 (0.044938)\n",
      "ExtraTreesClassifier: 0.572198 (0.055233)\n",
      "DecisionTreeClassifier: 0.576211 (0.064759)\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(random_state = seed)))\n",
    "models.append(('LASSO', Lasso())) \n",
    "models.append(('Ridge', Ridge())) \n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('KNeighborsClassifier', KNeighborsClassifier()))#ewights = 'distance' \n",
    "models.append(('XGBClassifier', xgb.XGBClassifier()))\n",
    "models.append(('GradientBoostingClassifier', GradientBoostingClassifier(random_state = seed)))\n",
    "models.append(('AdaBoostClassifier', AdaBoostClassifier(random_state = seed)))\n",
    "models.append(('RandomForestClassifier', RandomForestClassifier(random_state = seed)))\n",
    "models.append(('ExtraTreesClassifier', ExtraTreesClassifier(random_state = seed)))\n",
    "models.append(('DecisionTreeClassifier', DecisionTreeClassifier(random_state = seed)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(shuffle = True, n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring = scoring)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of feature: 85\n",
      "Feature Ranking: [ 1  1  1  1  4  1 10  1  1  1  1  1  1  1  1  1  1  1  1  9  1  1  1  1  1\n",
      " 11  5  1  1  1  3  1  1  8  1  6  7  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "estimator = LinearDiscriminantAnalysis()\n",
    "rfe = RFECV(estimator,cv = kfold)\n",
    "fit = rfe.fit(X,y)\n",
    "print(\"Num of feature: %d\") % fit.n_features_\n",
    "#print(\"Selected features: %s\") % fit.support_\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 first_kills_sum_dr_hist\n",
      "1 awp_kills_sum_dr_hist\n",
      "2 ADR_hist\n",
      "3 Train_total_played\n",
      "5 Cache_total_played\n",
      "7 Overpass_total_played\n",
      "8 Mirage_total_played\n",
      "9 Nuke_total_played\n",
      "10 Train_win_perc_map\n",
      "11 Cobblestone_win_perc_map\n",
      "12 Cache_win_perc_map\n",
      "13 Inferno_win_perc_map\n",
      "14 Overpass_win_perc_map\n",
      "15 Mirage_win_perc_map\n",
      "16 Nuke_win_perc_map\n",
      "17 rd_total_his_Ghost\n",
      "18 rd_total_his_OpTic\n",
      "20 rd_total_his_Luminosity\n",
      "21 rd_total_his_CLG\n",
      "22 rd_total_his_NRG\n",
      "23 rd_total_his_Cloud9\n",
      "24 rd_total_his_Immortals\n",
      "27 rd_total_his_Rogue\n",
      "28 rd_total_his_Misfits\n",
      "29 rd_total_his_Splyce\n",
      "31 total_team_rd_Cache\n",
      "32 total_team_rd_Cobblestone\n",
      "34 total_team_rd_Mirage\n",
      "37 total_team_rd_Nuke\n",
      "38 ADR_his_Cache\n",
      "39 ADR_his_Cobblestone\n",
      "40 ADR_his_Inferno\n",
      "41 ADR_his_Mirage\n",
      "42 ADR_his_Nuke\n",
      "43 ADR_his_Overpass\n",
      "44 ADR_his_Train\n",
      "45 first_kills_Cache_dr_hist\n",
      "46 first_kills_Cobblestone_dr_hist\n",
      "47 first_kills_Inferno_dr_hist\n",
      "48 first_kills_Mirage_dr_hist\n",
      "49 first_kills_Nuke_dr_hist\n",
      "50 first_kills_Overpass_dr_hist\n",
      "51 first_kills_Train_dr_hist\n",
      "52 awp_kills_Cache_dr_hist\n",
      "53 awp_kills_Cobblestone_dr_hist\n",
      "54 awp_kills_Inferno_dr_hist\n",
      "55 awp_kills_Mirage_dr_hist\n",
      "56 awp_kills_Nuke_dr_hist\n",
      "57 awp_kills_Overpass_dr_hist\n",
      "58 awp_kills_Train_dr_hist\n",
      "59 matches_played_player\n",
      "60 player_team_name_CLG\n",
      "61 player_team_name_Cloud9\n",
      "62 player_team_name_Ghost\n",
      "63 player_team_name_Immortals\n",
      "64 player_team_name_Liquid\n",
      "65 player_team_name_Luminosity\n",
      "66 player_team_name_Misfits\n",
      "67 player_team_name_NRG\n",
      "68 player_team_name_OpTic\n",
      "69 player_team_name_Renegades\n",
      "70 player_team_name_Rogue\n",
      "71 player_team_name_SK\n",
      "72 player_team_name_Splyce\n",
      "74 player_team_opponent_CLG\n",
      "75 player_team_opponent_Cloud9\n",
      "76 player_team_opponent_Ghost\n",
      "77 player_team_opponent_Immortals\n",
      "78 player_team_opponent_Liquid\n",
      "79 player_team_opponent_Luminosity\n",
      "80 player_team_opponent_Misfits\n",
      "81 player_team_opponent_NRG\n",
      "82 player_team_opponent_OpTic\n",
      "83 player_team_opponent_Renegades\n",
      "84 player_team_opponent_Rogue\n",
      "85 player_team_opponent_SK\n",
      "86 player_team_opponent_Splyce\n",
      "87 player_team_opponent_compLexity\n",
      "88 map_Cache\n",
      "89 map_Cobblestone\n",
      "90 map_Inferno\n",
      "91 map_Mirage\n",
      "92 map_Nuke\n",
      "93 map_Overpass\n",
      "94 map_Train\n"
     ]
    }
   ],
   "source": [
    "top_85_important_features = [] \n",
    "for b in range(0,len(fit.ranking_)):\n",
    "    if fit.ranking_[b] == 1:\n",
    "        top_85_important_features.append(b)\n",
    "        print b,data_adv.columns[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1054-3aae622ce6d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mensemble\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVotingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'soft'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m.69\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\model_selection\\_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                                               fit_params)\n\u001b[1;32m--> 140\u001b[1;33m                       for train, test in cv_iter)\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\model_selection\\_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    163\u001b[0m                 delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n\u001b[0;32m    164\u001b[0m                     sample_weight)\n\u001b[1;32m--> 165\u001b[1;33m                     for _, clf in self.estimators)\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    606\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.pyc\u001b[0m in \u001b[0;36m_parallel_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1028\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1029\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1082\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 787\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1030\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SuperBug\\Anaconda2\\envs\\cs_project\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for a in range(0, len(models)):\n",
    "    model1 = models[a]\n",
    "    for b in range(a+1, len(models)):\n",
    "        model2 = models[b]\n",
    "        for c in range(b+1, len(models)):\n",
    "            model3 = models[c]\n",
    "            estimators = []\n",
    "            estimators.append(model1)\n",
    "            estimators.append(model2)\n",
    "            estimators.append(model3)\n",
    "            ensemble = VotingClassifier(estimators, voting='soft')\n",
    "            results = model_selection.cross_val_score(ensemble, X, y, cv=kfold, scoring= scoring)\n",
    "            if results.mean() > .69:\n",
    "                print(model1[0], model2[0], model3[0],results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
