{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2, sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from IPython.core.display import HTML\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import urlparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = \"https://www.hltv.org/stats/matches/performance/mapstatsid/52325/immortals-vs-cloud9\"# performance page of match stats\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib2.Request(site,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page)\n",
    "r = requests.get(site, headers=hdr)\n",
    "tables = pd.read_html(r.text, header=0)\n",
    "total_team_kda = tables[0] # total kills deaths and assists of team\n",
    "who_kill_who = tables[1]  # who killed who\n",
    "first_kills = tables[2]  #first kill of the round\n",
    "awp_kills = tables[3]   #awp kills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "site= \"https://www.hltv.org/stats/matches/mapstatsid/52325/immortals-vs-cloud9\" #overview page of match stats\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib2.Request(site,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(page)\n",
    "  \n",
    "match_time = soup.find_all(\"div\", {\"class\":\"small-text\"})\n",
    "for item in match_time:\n",
    "    match_time = item.text\n",
    "match_time = datetime.datetime.strptime(match_time, '%Y-%m-%d  %H:%MMap') #match date and time\n",
    "\n",
    "r = requests.get(site, headers=hdr)\n",
    "tables = pd.read_html(r.text, header=0)\n",
    "team_a_stats, team_b_stats = tables[0], tables[1]\n",
    "\n",
    "round_history_team = soup.find_all(\"div\", class_ = \"round-history-team-row\") # winner of rounds and how rounds were won\n",
    "round_history_team_a = round_history_team[0].find_all(\"img\")\n",
    "round_history_team_b = round_history_team[1].find_all(\"img\")\n",
    "team_a_scores = []\n",
    "for scoreing in round_history_team_a:\n",
    "     team_a_scores.append([ scoreing.get('title')])\n",
    "team_b_scores = []                                                          #who won the round\n",
    "for scoreing in round_history_team_b:\n",
    "     team_b_scores.append([ scoreing.get('title')])\n",
    "team_a_ending = []                                                          #how the round was won\n",
    "for ending in round_history_team_a:\n",
    "    url = urlparse.urlparse(ending.get('src'))\n",
    "    base = os.path.basename(url.path)\n",
    "    team_a_ending.append([os.path.splitext(base)[0]])   \n",
    "team_b_ending = []\n",
    "for ending in round_history_team_b:\n",
    "    url = urlparse.urlparse(ending.get('src'))\n",
    "    base = os.path.basename(url.path)\n",
    "    team_b_ending.append([os.path.splitext(base)[0]])   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site= \"https://www.hltv.org/results?content=demo&team=7010&startDate=2016-09-01&endDate=2017-09-14\"\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib2.Request(site,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = soup.find_all(\"div\", class_='results-all')\n",
    "results = matches[0].find_all(\"a\", class_=\"a-reset\")\n",
    "urls1 = [result['href'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site= \"https://www.hltv.org/results?offset=100&startDate=2016-09-01&endDate=2017-09-14&content=demo&team=7010\"\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib2.Request(site,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = soup.find_all(\"div\", class_='results-all')\n",
    "results = matches[0].find_all(\"a\", class_=\"a-reset\")\n",
    "urls2 = [result['href'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = urls1 + urls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_match(site):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    site = 'https://www.hltv.org' + site\n",
    "    req = urllib2.Request(site,headers=hdr)\n",
    "    page = urllib2.urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "   \n",
    "    demo_url = 'https://www.hltv.org' + soup.find_all(\"a\", class_=\"flexbox left-right-padding\")[0]['href']\n",
    "    \n",
    "    try:\n",
    "        vetos = soup.find_all(\"div\", class_=\"standard-box veto-box\")[1].find_all(\"div\")[0].find_all(\"div\")\n",
    "        vetos = [veto.text for veto in vetos]\n",
    "    except:\n",
    "        vetos = None\n",
    "        \n",
    "    stats_url = 'https://www.hltv.org' + [a_element['href'] for a_element in soup.find_all(\"a\") if a_element.text == \"Detailed stats\"][0]\n",
    "\n",
    "    r = requests.get(site, headers=hdr)\n",
    "    tables = pd.read_html(r.text, header=0)\n",
    "    team_a, team_b = tables[0], tables[1]\n",
    "\n",
    "    return {\n",
    "        'url': site,\n",
    "        'vetos': vetos,\n",
    "        'stats_url': stats_url,\n",
    "        'teams': [team_a, team_b]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = []\n",
    "for idx, url in enumerate(urls):\n",
    "    matches.append(parse_match(url))\n",
    "    time.sleep(5)\n",
    "    print 'match {0} done'.format(idx)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
