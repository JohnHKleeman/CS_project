{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2, sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from IPython.core.display import HTML\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import urlparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site = \"https://www.hltv.org/stats/matches/heatmap/mapstatsid/52325/immortals-vs-cloud9?showKills=true&showDeaths=false&firstKillsOnly=false&allowEmpty=false&showKillDataset=true&showDeathDataset=true\"\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib2.Request(site,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page)\n",
    "r = requests.get(site, headers=hdr)\n",
    "tables = pd.read_html(r.text, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searching_data = soup.find_all( \"div\", class_ = \"heatmap-holder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = urllib2.Request(url,headers=hdr)\n",
    "    page = urllib2.urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "    return soup\n",
    "\n",
    "s = get_soup('https://www.hltv.org/stats/matches/heatmap/mapstatsid/52325/immortals-vs-cloud9?showKills=true&showDeaths=false&firstKillsOnly=false&allowEmpty=false&showKillDataset=true&showDeathDataset=true')\n",
    "\n",
    "divs = s.find_all('div', class_='heatmap heatmap-data')\n",
    "outer = divs[0]['data-heatmap-config']\n",
    "nested = divs[1]['data-heatmap-config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"renderAt\":\"uniqueHeatmapId1260517248\",\"heatmapData\":{\"data\":[{\"x\":286,\"y\":246,\"value\":1},{\"x\":489,\"y\":180,\"value\":1},{\"x\":360,\"y\":247,\"value\":1},{\"x\":493,\"y\":178,\"value\":1},{\"x\":584,\"y\":189,\"value\":1},{\"x\":454,\"y\":272,\"value\":1},{\"x\":453,\"y\":169,\"value\":1},{\"x\":459,\"y\":217,\"value\":1},{\"x\":265,\"y\":295,\"value\":1},{\"x\":261,\"y\":169,\"value\":1},{\"x\":269,\"y\":278,\"value\":1},{\"x\":411,\"y\":203,\"value\":1},{\"x\":226,\"y\":263,\"value\":1},{\"x\":366,\"y\":190,\"value\":1},{\"x\":278,\"y\":166,\"value\":1},{\"x\":436,\"y\":234,\"value\":1},{\"x\":406,\"y\":280,\"value\":1},{\"x\":270,\"y\":272,\"value\":1},{\"x\":342,\"y\":188,\"value\":1},{\"x\":338,\"y\":175,\"value\":1},{\"x\":198,\"y\":347,\"value\":1},{\"x\":276,\"y\":227,\"value\":1},{\"x\":417,\"y\":326,\"value\":1},{\"x\":381,\"y\":305,\"value\":1},{\"x\":367,\"y\":249,\"value\":1},{\"x\":228,\"y\":278,\"value\":1},{\"x\":271,\"y\":295,\"value\":1},{\"x\":514,\"y\":171,\"value\":1},{\"x\":505,\"y\":178,\"value\":1},{\"x\":592,\"y\":344,\"value\":1},{\"x\":584,\"y\":274,\"value\":1},{\"x\":532,\"y\":240,\"value\":1},{\"x\":296,\"y\":198,\"value\":1},{\"x\":311,\"y\":205,\"value\":1},{\"x\":303,\"y\":221,\"value\":1},{\"x\":392,\"y\":326,\"value\":1},{\"x\":547,\"y\":389,\"value\":1},{\"x\":270,\"y\":182,\"value\":1},{\"x\":293,\"y\":152,\"value\":1},{\"x\":175,\"y\":354,\"value\":1},{\"x\":541,\"y\":348,\"value\":1},{\"x\":507,\"y\":394,\"value\":1},{\"x\":371,\"y\":366,\"value\":1},{\"x\":257,\"y\":468,\"value\":1},{\"x\":255,\"y\":471,\"value\":1},{\"x\":363,\"y\":415,\"value\":1},{\"x\":574,\"y\":177,\"value\":1},{\"x\":574,\"y\":180,\"value\":1},{\"x\":408,\"y\":294,\"value\":1},{\"x\":471,\"y\":242,\"value\":1},{\"x\":433,\"y\":143,\"value\":1},{\"x\":448,\"y\":234,\"value\":1},{\"x\":492,\"y\":100,\"value\":1},{\"x\":396,\"y\":241,\"value\":1},{\"x\":383,\"y\":200,\"value\":1},{\"x\":264,\"y\":163,\"value\":1},{\"x\":254,\"y\":174,\"value\":1},{\"x\":539,\"y\":438,\"value\":1},{\"x\":274,\"y\":186,\"value\":1},{\"x\":500,\"y\":159,\"value\":1},{\"x\":322,\"y\":276,\"value\":1},{\"x\":256,\"y\":293,\"value\":1},{\"x\":267,\"y\":293,\"value\":1},{\"x\":419,\"y\":212,\"value\":1},{\"x\":359,\"y\":164,\"value\":1},{\"x\":284,\"y\":387,\"value\":1},{\"x\":561,\"y\":474,\"value\":1},{\"x\":420,\"y\":419,\"value\":1},{\"x\":471,\"y\":392,\"value\":1},{\"x\":426,\"y\":474,\"value\":1},{\"x\":509,\"y\":389,\"value\":1},{\"x\":277,\"y\":371,\"value\":1},{\"x\":288,\"y\":361,\"value\":1},{\"x\":492,\"y\":48,\"value\":1},{\"x\":269,\"y\":184,\"value\":1},{\"x\":444,\"y\":283,\"value\":1},{\"x\":555,\"y\":162,\"value\":1},{\"x\":553,\"y\":239,\"value\":1},{\"x\":241,\"y\":286,\"value\":1},{\"x\":555,\"y\":239,\"value\":1},{\"x\":343,\"y\":180,\"value\":1},{\"x\":264,\"y\":442,\"value\":1},{\"x\":269,\"y\":443,\"value\":1},{\"x\":279,\"y\":372,\"value\":1},{\"x\":502,\"y\":383,\"value\":1},{\"x\":323,\"y\":350,\"value\":1},{\"x\":500,\"y\":160,\"value\":1},{\"x\":539,\"y\":177,\"value\":1},{\"x\":490,\"y\":273,\"value\":1},{\"x\":228,\"y\":283,\"value\":1},{\"x\":583,\"y\":244,\"value\":1},{\"x\":505,\"y\":275,\"value\":1},{\"x\":233,\"y\":159,\"value\":1},{\"x\":305,\"y\":197,\"value\":1},{\"x\":312,\"y\":229,\"value\":1},{\"x\":336,\"y\":276,\"value\":1},{\"x\":248,\"y\":360,\"value\":1},{\"x\":503,\"y\":177,\"value\":1},{\"x\":458,\"y\":455,\"value\":1},{\"x\":509,\"y\":415,\"value\":1},{\"x\":277,\"y\":357,\"value\":1},{\"x\":264,\"y\":363,\"value\":1},{\"x\":429,\"y\":447,\"value\":1},{\"x\":309,\"y\":396,\"value\":1},{\"x\":308,\"y\":396,\"value\":1},{\"x\":283,\"y\":441,\"value\":1},{\"x\":241,\"y\":290,\"value\":1},{\"x\":451,\"y\":352,\"value\":1},{\"x\":394,\"y\":392,\"value\":1},{\"x\":244,\"y\":296,\"value\":1},{\"x\":249,\"y\":157,\"value\":1},{\"x\":221,\"y\":70,\"value\":1},{\"x\":489,\"y\":180,\"value\":1},{\"x\":319,\"y\":275,\"value\":1},{\"x\":530,\"y\":181,\"value\":1},{\"x\":516,\"y\":158,\"value\":1},{\"x\":309,\"y\":256,\"value\":1},{\"x\":507,\"y\":90,\"value\":1},{\"x\":502,\"y\":39,\"value\":1},{\"x\":321,\"y\":198,\"value\":1},{\"x\":274,\"y\":292,\"value\":1},{\"x\":310,\"y\":190,\"value\":1},{\"x\":455,\"y\":200,\"value\":1}],\"gradient\":{\".25\":\"rgb(0,0,255)\",\".55\":\"rgb(0,255,0)\",\".85\":\"yellow\",\"1\":\"yellow\"},\"radius\":10,\"max\":2},\"gradient\":{\".25\":\"rgb(0,0,255)\",\".55\":\"rgb(0,255,0)\",\".85\":\"yellow\",\"1\":\"yellow\"},\"radius\":10}'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stats_performance_page(url): \n",
    "    #site page example \"https://www.hltv.org/stats/matches/performance/mapstatsid/52325/immortals-vs-cloud9\"\n",
    "    # THIS THE STATS/PERFORMANCE PAGE\n",
    "    site = url\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    def site_soup(site):\n",
    "        req = urllib2.Request(site,headers=hdr)\n",
    "        page = urllib2.urlopen(req)\n",
    "        soup = BeautifulSoup(page)\n",
    "        return soup\n",
    "    def get_performance_data(soup,site):\n",
    "        r = requests.get(site, headers=hdr)\n",
    "        tables = pd.read_html(r.text, header=0)\n",
    "        total_team_kda = tables[0] \n",
    "        who_kill_who = tables[1] \n",
    "        first_kills = tables[2]  \n",
    "        awp_kills = tables[3]   \n",
    "        return {\n",
    "            'total_team_kda': total_team_kda, # total kills deaths and assists of team\n",
    "            'who_kill_who' : who_kill_who, # who killed who\n",
    "            'first_kills' : first_kills, #first kill of the round\n",
    "            'awp_kills' : awp_kills #awp kills\n",
    "        } \n",
    "    \n",
    "    soup = site_soup(site)\n",
    "    try:                         #this section decides whether or not its a 3 map 2 map or 1map series\n",
    "        match_3 = \"No match_3\"\n",
    "        best_of_three_data = soup.find_all(\"div\", class_ =\"columns\")[0]  # all dictated on whether or not\n",
    "        links = []                                                       #this find_all finds the unique 'columns'   \n",
    "        for link in best_of_three_data.find_all('a'):\n",
    "            links.append(link.get(\"href\"))\n",
    "        site1 = \"https://www.hltv.org\" + str(links[1])\n",
    "        site2 = \"https://www.hltv.org\" + str(links[2])\n",
    "        try:\n",
    "            site3 = \"https://www.hltv.org\" + str(links[3])\n",
    "            soup = site_soup(site3) \n",
    "            match_3 = get_performance_data(soup,site3)\n",
    "        finally:    \n",
    "            soup = site_soup(site1) \n",
    "            match_1 = get_performance_data(soup,site1)\n",
    "            soup = site_soup(site2) \n",
    "            match_2 = get_performance_data(soup,site2)\n",
    "            return { \n",
    "                \"match_1\": match_1,\n",
    "                \"match_2\": match_2,\n",
    "                \"match_3\": match_3\n",
    "            }\n",
    "    except:\n",
    "        match_1 = get_performance_data(soup, site)\n",
    "        return{\n",
    "            \"match_1\": match_1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stats_page(url): \n",
    "    #site page example  \"https://www.hltv.org/stats/matches/mapstatsid/52325/immortals-vs-cloud9\"\n",
    "    # THIS IS THE DETAILED STATS PAGE\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}   \n",
    "    site = url\n",
    "    def site_soup(site):\n",
    "        req = urllib2.Request(site,headers=hdr)\n",
    "        page = urllib2.urlopen(req)\n",
    "        soup = BeautifulSoup(page)\n",
    "        return soup\n",
    "\n",
    "    \n",
    "    def get_data_page(soup,site):\n",
    "        match_time = soup.find_all(\"div\", {\"class\":\"small-text\"})\n",
    "        for item in match_time:\n",
    "            match_time = item.text\n",
    "        match_time = datetime.datetime.strptime(match_time, '%Y-%m-%d  %H:%MMap') #match date and time\n",
    "\n",
    "        r = requests.get(site, headers=hdr)\n",
    "        tables = pd.read_html(r.text, header=0)\n",
    "        team_a_stats, team_b_stats = tables[0], tables[1]\n",
    "\n",
    "        round_history_team = soup.find_all(\"div\", class_ = \"round-history-team-row\") # winner of rounds and how rounds were won\n",
    "        round_history_team_a = round_history_team[0].find_all(\"img\")\n",
    "        round_history_team_b = round_history_team[1].find_all(\"img\")\n",
    "        team_a_scores = []\n",
    "        for scoreing in round_history_team_a:\n",
    "             team_a_scores.append([ scoreing.get('title')])                    #rounds that team a won\n",
    "        team_b_scores = []                                                         \n",
    "        for scoreing in round_history_team_b:                              \n",
    "             team_b_scores.append([ scoreing.get('title')])                    #rounds that team b won\n",
    "        team_a_ending = []                                                          \n",
    "        for ending in round_history_team_a:\n",
    "            url = urlparse.urlparse(ending.get('src'))\n",
    "            base = os.path.basename(url.path)                                  #how team a won the round\n",
    "            team_a_ending.append([os.path.splitext(base)[0]])   \n",
    "        team_b_ending = []\n",
    "        for ending in round_history_team_b:\n",
    "            url = urlparse.urlparse(ending.get('src'))\n",
    "            base = os.path.basename(url.path)\n",
    "            team_b_ending.append([os.path.splitext(base)[0]])                  #how team b won the round\n",
    "        return {\n",
    "            'match_time' : match_time, #match date and time\n",
    "            'team_scores': [team_a_scores, team_b_scores], #rounds that team a won\n",
    "            'team_endings': [team_a_ending, team_b_ending] #how the team won the round\n",
    "           }\n",
    "    \n",
    "    soup = site_soup(site)\n",
    "   \n",
    "    try:             #this section decides whether or not its a 3 map 2 map or 1map series\n",
    "        match_3 = \"No match_3\"\n",
    "        best_of_three_data = soup.find_all(\"div\", class_ =\"columns\")[0] # all dictated on whether or not\n",
    "        links = []                                                    #this find_all finds the unique 'columns'   \n",
    "        for link in best_of_three_data.find_all('a'):           \n",
    "            links.append(link.get(\"href\"))\n",
    "        site1 = \"https://www.hltv.org\" + str(links[1])\n",
    "        site2 = \"https://www.hltv.org\" + str(links[2])\n",
    "        try:\n",
    "            site3 = \"https://www.hltv.org\" + str(links[3])\n",
    "            soup = site_soup(site3) \n",
    "            match_3 = get_data_page(soup,site3)\n",
    "        finally:    \n",
    "            soup = site_soup(site1) \n",
    "            match_1 = get_data_page(soup,site1)\n",
    "            soup = site_soup(site2) \n",
    "            match_2 = get_data_page(soup,site2)\n",
    "            return { \n",
    "                \"match_1\": match_1,\n",
    "                \"match_2\": match_2,\n",
    "                \"match_3\": match_3\n",
    "            }\n",
    "    except:\n",
    "        match_1 = get_data_page(soup, site)\n",
    "        return{\n",
    "            \"match_1\": match_1\n",
    "        }        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site= \"https://www.hltv.org/results?content=demo&team=7010&startDate=2016-09-01&endDate=2017-09-14\"\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib2.Request(site,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = soup.find_all(\"div\", class_='results-all')\n",
    "results = matches[0].find_all(\"a\", class_=\"a-reset\")\n",
    "urls1 = [result['href'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site= \"https://www.hltv.org/results?offset=100&startDate=2016-09-01&endDate=2017-09-14&content=demo&team=7010\"\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib2.Request(site,headers=hdr)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = soup.find_all(\"div\", class_='results-all')\n",
    "results = matches[0].find_all(\"a\", class_=\"a-reset\")\n",
    "urls2 = [result['href'] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = urls1 + urls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_match(site):\n",
    "   #site page example https://www.hltv.org/matches/2315168/cloud9-vs-nrg-epicenter-2017-americas-closed-qualifier\n",
    "    # THIS IS THE MATCH STATS PAGE\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    site = 'https://www.hltv.org' + site\n",
    "    req = urllib2.Request(site,headers=hdr)\n",
    "    page = urllib2.urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "   \n",
    "    demo_url = 'https://www.hltv.org' + soup.find_all(\"a\", class_=\"flexbox left-right-padding\")[0]['href']\n",
    "   \n",
    "    try:\n",
    "        vetos = soup.find_all(\"div\", class_=\"standard-box veto-box\")[1].find_all(\"div\")[0].find_all(\"div\")\n",
    "        vetos = [veto.text for veto in vetos]\n",
    "    except:\n",
    "        vetos = None\n",
    "        \n",
    "        \n",
    "    team_name = soup.find_all(\"div\", class_=\"standard-box teamsBox\")[0].find_all(\"div\", class_ = \"teamName\")\n",
    "    team_info = soup.find_all(\"div\", class_=\"flexbox fix-half-width-margin maps\")[0].find_all(\"div\", class_ = \"results\")\n",
    "\n",
    "    for name in team_name[0]:\n",
    "        team_a_name = name\n",
    "    for name in team_name[1]:\n",
    "        team_b_name = name\n",
    "    best_of = soup.find_all(\"div\", class_ = \"padding preformatted-text\")\n",
    "    for best in best_of:\n",
    "        best_of = best.text\n",
    "    if \"Best of 3\" in best_of:\n",
    "        best_of = \"Best of 3\"\n",
    "        map_name_one = soup.find_all(\"div\", class_ = \"mapname\")[0]\n",
    "        for name in map_name_one:\n",
    "            map_name_one = name\n",
    "        map_name_two = soup.find_all(\"div\", class_ = \"mapname\")[1]\n",
    "        for name in map_name_two:\n",
    "            map_name_two = name\n",
    "        map_name_three = soup.find_all(\"div\", class_ = \"mapname\")[2]\n",
    "        for name in map_name_three:\n",
    "            map_name_three = name\n",
    "\n",
    "\n",
    "        team_a_score_map_one = team_info[0].find_all(\"span\")[0].text\n",
    "        team_b_score_map_one = team_info[0].find_all(\"span\")[2].text\n",
    "        team_a_score_map_two = team_info[1].find_all(\"span\")[0].text\n",
    "        team_b_score_map_two = team_info[1].find_all(\"span\")[2].text\n",
    "        try:\n",
    "            team_a_score_map_three = team_info[2].find_all(\"span\")[0].text\n",
    "            team_b_score_map_three = team_info[2].find_all(\"span\")[2].text\n",
    "\n",
    "        except: \n",
    "            team_b_score_map_three = 'NA'\n",
    "            team_a_score_map_three = 'NA'\n",
    "        \n",
    "       \n",
    "    elif \"Best of 1\" in best_of:\n",
    "        best_of = \"Best of 1\"\n",
    "        map_name_one = soup.find_all(\"div\", class_ = \"mapname\")[0]\n",
    "        for name in map_name_one:\n",
    "            map_name_one = name\n",
    "        team_a_score_map_one = team_info[0].find_all(\"span\")[0].text\n",
    "        team_b_score_map_one = team_info[0].find_all(\"span\")[2].text\n",
    "        map_name_two = team_a_score_map_two = team_b_score_map_two = map_name_three = team_a_score_map_three = team_b_score_map_three = 'NA'\n",
    "       \n",
    "    else:\n",
    "        print(\"new type of match format\")\n",
    "\n",
    "    \n",
    "    stats_url = 'https://www.hltv.org' + [a_element['href'] for a_element in soup.find_all(\"a\") if a_element.text == \"Detailed stats\"][0]\n",
    "\n",
    "    r = requests.get(site, headers=hdr)\n",
    "    tables = pd.read_html(r.text, header=0)\n",
    "    team_a, team_b = tables[0], tables[1]\n",
    "\n",
    "    return {\n",
    "        'team_a_b' : (team_a_name, team_b_name),\n",
    "        'map_one': (map_name_one, team_a_score_map_one, team_b_score_map_one),\n",
    "        'map_two': (map_name_two, team_a_score_map_two, team_b_score_map_two),\n",
    "        'map_three': (map_name_three, team_a_score_map_three, team_b_score_map_three),\n",
    "        'url': site,\n",
    "        'vetos': vetos,\n",
    "        'stats_url': stats_url,\n",
    "        'teams': [team_a, team_b]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = []\n",
    "for idx, url in enumerate(urls):\n",
    "    matches.append(parse_match(url))\n",
    "    time.sleep(5)\n",
    "    print 'match {0} done'.format(idx)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
