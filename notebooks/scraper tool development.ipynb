{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2, sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from IPython.core.display import HTML\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_teamID(team_name):\n",
    "    team_info = pd.read_csv('../../scrapped_data/team_info.csv', index_col=0)\n",
    "    teamID = team_info.loc[team_info.team_name == team_name, 'team_id'].tolist()[0]\n",
    "    return teamID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = urllib2.Request(url,headers=hdr)\n",
    "    page = urllib2.urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_match_urls(params):\n",
    "    done=False\n",
    "    params['offset'] = 0\n",
    "    urls = []\n",
    "    while not done:\n",
    "        match_page = \"https://www.hltv.org/results?offset={offset}&content=demo&team={teamID}&startDate={startDate}&endDate={endDate}\".format(**params)\n",
    "        soup = get_soup(match_page)\n",
    "        matches = soup.find_all(\"div\", class_='results-all')\n",
    "        \n",
    "        if len(matches) == 0:\n",
    "            break\n",
    "        \n",
    "        results = matches[0].find_all(\"a\", class_=\"a-reset\")\n",
    "        urls  += [result['href'] for result in results]\n",
    "        if len(urls) % 100 != 0:\n",
    "            done = True\n",
    "        else:\n",
    "            params['offset'] += 100\n",
    "    del params['offset']\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_match(site):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    site = 'https://www.hltv.org' + site\n",
    "    req = urllib2.Request(site,headers=hdr)\n",
    "    page = urllib2.urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "    req = urllib2.Request(site,headers=hdr)\n",
    "    page = urllib2.urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "\n",
    "    demo_url = 'https://www.hltv.org' + soup.find_all(\"a\", class_=\"flexbox left-right-padding\")[0]['href']\n",
    "    \n",
    "    try:\n",
    "        vetos = soup.find_all(\"div\", class_=\"standard-box veto-box\")[1].find_all(\"div\")[0].find_all(\"div\")\n",
    "        vetos = [veto.text for veto in vetos]\n",
    "    except:\n",
    "        vetos = None\n",
    "        \n",
    "    stats_url = 'https://www.hltv.org' + [a_element['href'] for a_element in soup.find_all(\"a\") if a_element.text == \"Detailed stats\"][0]\n",
    "\n",
    "    tables = soup.find_all(\"div\", class_ = \"stats-content\", id=\"all-content\")[0]\n",
    "    tables = tables.find_all(\"table\")\n",
    "    r = requests.get(site, headers=hdr)\n",
    "    tables = pd.read_html(r.text, header=0)\n",
    "    team_a, team_b = tables[0], tables[1]\n",
    "\n",
    "    return {\n",
    "        'url': site,\n",
    "        'vetos': vetos,\n",
    "        'stats_url': stats_url,\n",
    "        'teams': [team_a, team_b]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_match_data(team_name, startDate, endDate):\n",
    "    teamID = get_teamID(team_name)\n",
    "    params = {\n",
    "        'teamID':teamID,\n",
    "        'startDate':startDate,\n",
    "        'endDate':endDate\n",
    "    }\n",
    "    urls = get_match_urls(params)\n",
    "    matches = []\n",
    "    for idx, url in enumerate(urls):\n",
    "        matches.append(parse_match(url))\n",
    "        time.sleep(5)\n",
    "        print 'match {0} done'.format(idx)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing function\n",
    "\n",
    "team_name = 'TyLoo'\n",
    "startDate ='2017-08-01'\n",
    "endDate ='2017-08-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
